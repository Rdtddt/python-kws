### 웹크롤링
#! 1. Web crawling
# 웹페이지를 체계적으로 탐색하여 원하는 데이터를 추출하는 행위
# 웹 크롤러 - 크롤링 수행하는 프로그램
#! 2. 목적
# 2-1 검색엔진 최적화(SEO)
# 웹 크롤러는 검색엔진에서 웹페이지의 내용을 인덱싱(추출) 하기 위해 사용
# 2-2. 데이터 수집
# 특정 웹 사이트에서 정보 수집
# 2-3. 시장조사, 컨텐츠 모니터링
#! 3. 파이썬을 이용한 웹 크롤링
# : 파이썬 외부 라이브러리의 지원(BeautifulSoup, Scrapy), 확장성

#! 4. http의 기본이해
# HyperText Trasnfer protocol
# 웹에서 데이터를 주고받는 규약

# 4-1. Http와 웹크롤링의 관계
# 웹 크롤링 시 웹 서버에게 정보를 요청하고, 그 응답을 받아오는 과정에서 Http를 사용

# 4-2. http 요청과 응답
# 요청(request): 웹브라우저(크롤러) 가 서버에게 정보나 서비스를 요청하는 메시지
# 응답(Response): 서버가 요청을 처리한 후 반환하는 메시지

# 4-3. http메서드
# GET: 웹페이지의 내용을 조회할 때 사용, 웹 크롤링에서 가장 기본적인 요청 방식
# POST: 서버에 데이터를 보낼 때 사용

# 4-4. HTTP: 상태코드(크롤링 중 발생할 수 있는 상태)
# 200 OK: 성공적으로 응답받음
# 400 Forbidden: 접근권한이 없음 (로봇 차단 정책 등으로 크롤링 제한)
# 404 Not found: 요청한 URL에 해당하는 페이지가 없음.(링크가 끊긴 페이지 참조 시 발생)

#! User-Agent
# : 웹 브라우저니 기타 클라이언트가 서버에게 자신의 신분을 알리기 위해
# 500 : HTTP 요청 헤더에 포함시키는 문자열

#! robots.txt
# 웹사이트의 어느 부분을 수집하거나 안해야 하는지에 대한 지침 제공
# User-agent: *(*는 모든 크롤러에게 적용)
# Disallow: /private/
# Disallow: /
